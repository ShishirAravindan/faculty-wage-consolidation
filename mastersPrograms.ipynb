{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.mastersportal.com/search/master/united-states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTag(URL, tagType, tagClass, timeout=10):\n",
    "    options = Options()\n",
    "    options.set_preference(\"dom.popup_maximum\", 0)\n",
    "    options.set_preference(\"privacy.popups.disable_from_plugins\", 3)\n",
    "    \n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    # Navigate to the page\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        # Wait for the dynamic content to load (adjust the timeout as needed)\n",
    "        disciplines = WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.XPATH, f\"//{tagType}[@class='{tagClass}']\"))\n",
    "        )\n",
    "\n",
    "        # Extract the data\n",
    "        scraped_data = disciplines.text\n",
    "        print(scraped_data)\n",
    "\n",
    "    finally:\n",
    "        # Close the browser window\n",
    "        driver.quit()\n",
    "    return scraped_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCIPLINES = extractTag(URL, \"section\", \"DisciplineFilterWrapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCIPLINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedUpDisciplines = DISCIPLINES.split('\\n')[1::2]\n",
    "with open('ALL_DISCIPLINES.json', 'w') as f:\n",
    "    json.dump(cleanedUpDisciplines, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetURL = \"https://www.mastersportal.com/search/master/business-management/united-states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessDISCIPLINES = extractTag(TargetURL, \"section\", \"DisciplineFilterWrapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businessDISCIPLINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedUpSubDisciplines = businessDISCIPLINES.split('\\n')[5::2]\n",
    "with open('BUSINESS_DISCIPLINES.json', 'w') as f:\n",
    "    json.dump(cleanedUpSubDisciplines, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running for business disciplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertisingURL = \"https://www.mastersportal.com/search/master/advertising/united-states\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allProgramsInfo = extractTag(advertisingURL, \"ul\", \"SearchResultsList\", timeout=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extractSinglePrograms(allProgramInfoList):\n",
    "    programInfoCleaned = []\n",
    "    currProgram = []\n",
    "\n",
    "    for item in allProgramInfoList:\n",
    "        if \"Add to compare\" in item:\n",
    "            if currProgram:\n",
    "                programInfoCleaned.append(currProgram)\n",
    "                currProgram = []\n",
    "        else:\n",
    "           if item != 'Check match': currProgram.append(item) \n",
    "\n",
    "    if currProgram: programInfoCleaned.append(currProgram)\n",
    "    return programInfoCleaned\n",
    "\n",
    "def processProgramInfo(rawProgramInfo):\n",
    "    cleanedUpProgramInfo = rawProgramInfo.split('\\n')\n",
    "    cleanedUpIndividualPrograms = _extractSinglePrograms(cleanedUpProgramInfo)\n",
    "    return cleanedUpIndividualPrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = processProgramInfo(allProgramsInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevantProgramInfo = [sublist[:7] for sublist in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountingPageOneDF = pd.DataFrame(relevantProgramInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountingPageOneDF.columns=[\"Program\", \"Tuition\", \"Duration\", \"Description\", \n",
    "                             \"Program Type\", \"University\", \"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountingPageOneDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountingPageOneDF.to_json('account_1.json', orient='records')\n",
    "accountingPageOneDF.to_excel('advertising_pageOne.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageCount = int(extractTag(advertisingURL, \"p\", \"SeeMoreLabelVar1\", timeout=5)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allProgramsInfoRawList = []\n",
    "for pageIndex in range(1, pageCount):\n",
    "    currPageURL = f\"{advertisingURL}/page={pageIndex + 1}\" if pageIndex > 0 else advertisingURL\n",
    "    allProgramsInfoRawList.extend(extractTag(currPageURL, \"ul\", \"SearchResultsList\", timeout=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import time\n",
    "\n",
    "def extractTagWithPagination(URL, tagType, tagClass, next_button_xpath, timeout=10, max_pages=None):\n",
    "    options = Options()\n",
    "    options.set_preference(\"dom.popup_maximum\", 0)\n",
    "    options.set_preference(\"privacy.popups.disable_from_plugins\", 3)\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    # Navigate to the page\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        print(driver.current_url)\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            next_button = WebDriverWait(driver, timeout).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"NextButton\"))\n",
    "                )\n",
    "            # Wait for the dynamic content to load (adjust the timeout as needed)\n",
    "            disciplines = WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.XPATH, f\"//{tagType}[@class='{tagClass}']\"))\n",
    "            )\n",
    "\n",
    "            # Extract the data\n",
    "            scraped_data = disciplines.text\n",
    "            print(f\"Page {page_number}:\", scraped_data, \"\\n\")\n",
    "            print(f\"Scraped Page {page_number} \\n\")\n",
    "\n",
    "            # Check if there's a next button\n",
    "            try:\n",
    "                next_button.click()\n",
    "                page_number += 1\n",
    "\n",
    "                time.sleep(2)\n",
    "            except TimeoutException:\n",
    "                # Break the loop if there's no next button (reached the last page)\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "\n",
    "            if max_pages is not None and page_number >= max_pages:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        scraped_data += disciplines.text\n",
    "        # Close the browser window\n",
    "        driver.quit()\n",
    "\n",
    "    return scraped_data\n",
    "\n",
    "# Example usage:\n",
    "next_button_xpath = \"//button[@class='NextButton']\"  # Update with the correct XPath for the next button\n",
    "max_pages = 2  # Set to None if you want to scrape all pages\n",
    "\n",
    "a = extractTagWithPagination(advertisingURL, \"ul\", \"SearchResultsList\", next_button_xpath, max_pages=max_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allProgramsInfoRawList = processProgramInfo(allProgramsInfo)\n",
    "allProgramsInfoRawList = [sublist[:7] for sublist in allProgramsInfoRawList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountingDF = pd.DataFrame(relevantProgramInfo)\n",
    "accountingDF.columns=[\"Program\", \"Tuition\", \"Duration\", \"Description\", \n",
    "                             \"Program Type\", \"University\", \"Location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountingDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeGeoLoc(URL, tagType, tagClass, timeout=10):\n",
    "\n",
    "    # Set the latitude and longitude for the desired geolocation\n",
    "    location = {'latitude': 40.7128, 'longitude': -74.0060}\n",
    "\n",
    "    firefox_options = webdriver.FirefoxOptions()\n",
    "\n",
    "    # Enable geolocation\n",
    "    firefox_options.set_preference(\"geo.enabled\", True)\n",
    "\n",
    "    # Set the geolocation coordinates using JavaScript\n",
    "    firefox_options.set_preference(\"geo.provider.network.url\", \n",
    "        f\"data:application/json, {{\\\"location\\\": {{\\\"lat\\\": {location['latitude']}, \\\"lng\\\": {location['longitude']}}}}}\")\n",
    "\n",
    "\n",
    "    # Create a Firefox WebDriver with the configured options\n",
    "    driver = webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "    # Example usage: Open a website to check geolocation\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        # Wait for the dynamic content to load (adjust the timeout as needed)\n",
    "        disciplines = WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.XPATH, f\"//{tagType}[@class='{tagClass}']\"))\n",
    "        )\n",
    "\n",
    "        # Extract the data\n",
    "        scraped_data = disciplines.text\n",
    "        print(scraped_data)\n",
    "\n",
    "\n",
    "    finally:\n",
    "        # Close the browser window\n",
    "        driver.quit()\n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YRL = \"https://www.mastersportal.com/search/master/actuarial-science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2(URL, tagType, tagClass, nextButtonClass, timeout=10, isHeadless=False):\n",
    "    \"\"\"\n",
    "    Extracts data from a paginated web page with dynamic content using Selenium.\n",
    "\n",
    "    Args:\n",
    "        URL (str): The URL of the web page to scrape.\n",
    "        tagType (str): The HTML tag type (e.g., 'ul', 'div') containing the data.\n",
    "        tagClass (str): The class attribute value of the HTML tag containing the data.\n",
    "        nextButtonClass (str): The class attribute value of the 'Next' button for pagination.\n",
    "        timeout (int, optional): Maximum time to wait for elements to load in seconds (default is 10).\n",
    "        isHeadless (bool, optional): Option to set the browser in headless mode. Set to False by default.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated string containing the scraped data from all pages.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses a headless Firefox browser for scraping.\n",
    "        - Adjust the 'timeout' parameter as needed based on the page load time.\n",
    "        - It's recommended to set a reasonable delay using 'time.sleep()' to allow page load before scraping.\n",
    "    \"\"\"\n",
    "    scraped_data = ''  # initializing a string accumulator\n",
    "\n",
    "    options = Options()\n",
    "    options.set_preference(\"dom.popup_maximum\", 0)\n",
    "    options.set_preference(\"privacy.popups.disable_from_plugins\", 3)\n",
    "\n",
    "    if isHeadless:\n",
    "        options.add_argument(\"--headless\")  # Setting browser to headless mode\n",
    "\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    driver.get(URL)  # Navigate to the page\n",
    "\n",
    "    try:\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            # Wait for the dynamic content to load (adjust the timeout as needed)\n",
    "            disciplines = WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.XPATH, f\"//{tagType}[@class='{tagClass}']\"))\n",
    "            )\n",
    "\n",
    "            # Extract the data\n",
    "            scraped_data += disciplines.text + '\\n'\n",
    "            print(f\"Scraped Page {page_number} \\n\")\n",
    "\n",
    "            # Check if there's a next button and if it's enabled\n",
    "            try:\n",
    "                next_button = WebDriverWait(driver, timeout).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, nextButtonClass))\n",
    "                )\n",
    "                if not next_button.is_enabled():\n",
    "                    print(\"Next button is disabled. End of pagination.\")\n",
    "                    break\n",
    "\n",
    "                next_button.click()\n",
    "                page_number += 1\n",
    "\n",
    "                time.sleep(2)\n",
    "            except TimeoutException:\n",
    "                # Break the loop if there's no next button (reached the last page)\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        # Close the browser window\n",
    "        driver.quit()\n",
    "\n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2(YRL, \"ul\", \"SearchResultsList\", \"NextButton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTagWithPagination(URL, tagType, tagClass, nextButtonClass, timeout=10, max_pages=None, isHeadless=False):\n",
    "    \"\"\"\n",
    "    Extracts data from a paginated web page with dynamic content using Selenium.\n",
    "\n",
    "    Args:\n",
    "        URL (str): The URL of the web page to scrape.\n",
    "        tagType (str): The HTML tag type (e.g., 'ul', 'div') containing the data.\n",
    "        tagClass (str): The class attribute value of the HTML tag containing the data.\n",
    "        nextButtonClass (str): The class attribute value of the 'Next' button for pagination.\n",
    "        timeout (int, optional): Maximum time to wait for elements to load in seconds (default is 10).\n",
    "        max_pages (int, optional): Maximum number of pages to scrape. Set to None to scrape all pages.\n",
    "        isHeadless (bool, optional): Option to set the browser in headless mode. Set to False by default.\n",
    "\n",
    "    Returns:\n",
    "        str: Concatenated string containing the scraped data from all pages.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses a headless Firefox browser for scraping.\n",
    "        - Adjust the 'timeout' parameter as needed based on the page load time.\n",
    "        - It's recommended to set a reasonable delay using 'time.sleep()' to allow page load before scraping.\n",
    "    \"\"\"\n",
    "    scraped_data = '' #initializing a string accumulator\n",
    "\n",
    "\n",
    "    options = Options()\n",
    "    options.set_preference(\"dom.popup_maximum\", 0)\n",
    "    options.set_preference(\"privacy.popups.disable_from_plugins\", 3)\n",
    "\n",
    "    if isHeadless: options.add_argument(\"--headless\") # Setting browser to headless mode\n",
    "    \n",
    "    driver = webdriver.Firefox(options=options)\n",
    "\n",
    "    driver.get(URL) # Navigate to the page\n",
    "\n",
    "    try:\n",
    "        page_number = 1\n",
    "        while True:\n",
    "            next_button = WebDriverWait(driver, timeout).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"NextButton\"))\n",
    "                )\n",
    "            # Wait for the dynamic content to load (adjust the timeout as needed)\n",
    "            disciplines = WebDriverWait(driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.XPATH, f\"//{tagType}[@class='{tagClass}']\"))\n",
    "            )\n",
    "\n",
    "            # Extract the data\n",
    "            scraped_data += disciplines.text + '\\n'\n",
    "            print(f\"Scraped Page {page_number} \\n\")\n",
    "\n",
    "            # Check if there's a next button\n",
    "            try:\n",
    "                next_button.click()\n",
    "                page_number += 1\n",
    "\n",
    "                time.sleep(2)\n",
    "            except TimeoutException:\n",
    "                # Break the loop if there's no next button (reached the last page)\n",
    "                print(\"EOF\")\n",
    "                break\n",
    "\n",
    "            if max_pages is not None and page_number >= max_pages:\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        scraped_data += disciplines.text\n",
    "        print(f\"Scraped Page {page_number} \\n\")\n",
    "        # Close the browser window\n",
    "        driver.quit()\n",
    "\n",
    "    return scraped_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".finPayVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ffcf1355783521936c8a93fbbdf23c3da764a9fb54c986f26237c920f75f412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
